# -*- coding: utf-8 -*-
"""issue159_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vt2Lx04kP1ue0CpylQf7Fbf2jNLaceZ7
"""



'''def __scrape_page(self):
     username = self.username
     data = requests.get(f"https://github.com/{username}")
     data = BeautifulSoup(data.text, "html.parser")
     return data'''

'''def __get_repo_page(self):
        """
         Scrape the repositories page of a GitHub user.
        """
        username = self.username
        repo_data = requests.get(f"https://github.com/{username}?tab=repositories")
        repo_data = BeautifulSoup(repo_data.text, "html.parser")
        return repo_data'''

def get_all_repo_details(self):

    page = self.__get_repo_page()

    repo_elements = page.select('#user-repositories-list ul li')



    repositories = []

    for repo_element in repo_elements:

        link = repo_element.find('a')['href']
        repositories.append(f'https://github.com{link}')

        name = repo_element.find('a').text.split('/')[-1]
        repositories.append(name.replace('\n', ''))

        description = repo_element.find('p').get_text(strip=True) if repo_element.find('p') else None
        #description= description.replace('\n','')
        repositories.append(description)

        url1= 'https://github.com' + link
        response1= requests.get(url1)
        soup= BeautifulSoup(response1.content, 'html.parser')
        li_elements = soup.find_all('li', class_= "d-inline")

        for li in li_elements:
          language = li.text.strip()
          repositories.append(language.replace('\n',''))

          stars= soup.find('span', class_='text-bold')
          num_of_stars = stars.text if stars else 'N/A'
          repositories.append(num_of_stars.replace('\n',''))

        pullurl= url1 + '/pulls'
        issuesurl= url1+'/issues'
        pullresponse= requests.get(pullurl) #getting the content of pull requests page
        issueresponse= requests.get(issuesurl) #getting the content of issues page

        p_soup= BeautifulSoup(pullresponse.content, 'html.parser') #creating beautifulsoup object for pull requests page
        i_soup= BeautifulSoup(issueresponse.content, 'html.parser') #creating beautifulsoup object for issues page

        #to find number of pull requests
        pullrequests= p_soup.find('div', class_="table-list-header-toggle states flex-auto pl-0" )
        num_of_pull_requests= pullrequests.text.strip() if pullrequests else 'N/A'
        repositories.append(num_of_pull_requests.replace('\n',''))

        issues= i_soup.select_one('span#issues-repo-tab-count')
        num_of_issues= issues.text.strip() if issues else 'N/A'
        repositories.append(num_of_issues.replace('\n',''))


    return repositories







